{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:01:51.977876200Z",
     "start_time": "2023-12-22T07:01:51.963914300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:10.826268Z",
     "start_time": "2023-12-22T07:02:10.284715700Z"
    }
   },
   "id": "a57a89a3b997bc46"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 100, 100, 8024, 5042, 4917, 100, 8024, 3221, 2769, 812, 5632, 4777, 4638, 6427, 6241, 3563, 1798, 722, 671, 102], [101, 8024, 712, 6206, 3221, 711, 100, 5310, 3354, 6392, 6369, 749, 3173, 4638, 3181, 6760, 2466, 855, 5390, 5356, 4772, 8020, 100, 100, 100, 8024, 100, 8021, 102], [101, 511, 100, 1072, 3300, 5679, 1962, 4638, 4415, 6389, 2595, 6574, 8024, 684, 3221, 4680, 1184, 1546, 671, 671, 4905, 1377, 809, 2418, 4500, 1168, 5296, 2595, 100, 4638, 5318, 2190, 855, 5390, 5356, 4772, 8024, 4680, 1184, 3341, 4692, 2141, 7741, 5310, 3362, 738, 7567, 711, 679, 7231, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer([\"Rotary Transformer，简称RoFormer，是我们自研的语言模型之一\",\"，主要是为Transformer结构设计了新的旋转式位置编码（Rotary Position Embedding，RoPE）\",\"。RoPE具有良好的理论性质，且是目前唯一一种可以应用到线性Attention的绝对位置编码，目前来看实验结果也颇为不错。\"],max_length=107)\n",
    "print(encoded_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:12.430977300Z",
     "start_time": "2023-12-22T07:02:12.362160600Z"
    }
   },
   "id": "5f0fe1db8c988140"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  label\n",
      "0      [衰][衰][衰]回复@ciciwithyou:没事啦，来得及的，搞个小抽奖，准备一两百套衣...      0\n",
      "1      【#超级笑星# 精彩剧照】[哈哈]我们超级可爱的小球童在现场人气也很高哦~[嘻嘻]不仅有@主...      0\n",
      "2      1、和移动的“梦网”服务的联系太紧密了，到处都是“移动梦网”的快捷键。2、不支持自动开机，不...      0\n",
      "3      关注[哈哈]记者就是记者，眼神就是好@北京菠菜 太恐怖了 抱考拉摸袋鼠，又完成了2个心愿，好...      0\n",
      "4      见过悲催的，没见过这么悲催的。大晚上等公车半个多小时才来；快到家了才想起来没带钥匙。只好乖乖...      0\n",
      "...                                                  ...    ...\n",
      "42959    以前刚有了孩子看过一本卡尔威特的教育，感觉要比这本书好多了，我翻了个大概，就扔到一边去了。\\n      0\n",
      "42960             谁知道燕麦粥用什么办法能煮成像酒店那样啊？最近想喝可是做出来很失败[晕]\\n      0\n",
      "42961  充话费得的...现在父母的想象力远远超过我们的父母啊?我小时候说是路边捡的，害我每次被妈妈骂...      0\n",
      "42962           发货找货包装和快递蛋逼这事比大便干燥还难受憋屈！[泪]@代购JOaNLOLo\\n      0\n",
      "42963   现在风越来越大了…[抓狂] 泪！ 北京出租车是北京城市的耻辱！宰客，拒载，乱象丛生！！[怒]\\n      0\n",
      "\n",
      "[42964 rows x 2 columns]\n",
      "                                                 comment  label\n",
      "0      买书前在网上看完了开放的部分，心里念念不忘，每看到书名都能引起联想，放不下的一本书，一般网络...      1\n",
      "1      penny是骰子，一场麻将下来，人人都有至少抓一把的机会最爱《盛夏光年》@Chloe的呼吸 ...      1\n",
      "2                                   幸福如此简单[爱你][爱你][爱你]\\n      1\n",
      "3      谢谢莫莫 我就怕你戴会小，刚刚好我就放心啦...美美的过年吧...蜜蜜小姐新年快乐思密达.....      1\n",
      "4      回复@MMaiH:这个是花一晚上的钱，住两晚上！[哈哈] 回复@磨西春天酒吧:好的哈[哈哈]...      1\n",
      "...                                                  ...    ...\n",
      "42959  年年岁岁开心如花 遥祝@花姐爱遛遛 付帅你真用心[赞][赞] 山东琴书，好多年没在舞台上见到...      1\n",
      "42960          还有专业版微博 →_→ [哈哈]快来争当有知识有文化的专业吃货！转发竞猜有奖哦\\n      1\n",
      "42961                                             [鼓掌]\\n      1\n",
      "42962  只注意大客了![哈哈]都不是我的菜。哈哈哈。【#超级笑星# 精彩剧照】还记得古灵精怪又迷人的...      1\n",
      "42963                   还可以吧。给朋友买的，电器方面我都京东买。还没发现什么问题。\\n      1\n",
      "\n",
      "[42964 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_texts_orig = []\n",
    "neg=[]\n",
    "pos=[]\n",
    "with open('neg.txt','r',encoding='utf-8') as reader:\n",
    "    for row in reader:\n",
    "        neg.append(row)\n",
    "data_neg=pd.DataFrame(np.array(neg),columns=['comment'])\n",
    "data_neg['label']=0\n",
    "print(data_neg)\n",
    "with open('pos.txt','r',encoding='utf-8') as reader:\n",
    "    for row in reader:\n",
    "        pos.append(row)\n",
    "data_pos=pd.DataFrame(np.array(pos),columns=['comment'])\n",
    "data_pos['label']=1\n",
    "print(data_pos)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:33.715952900Z",
     "start_time": "2023-12-22T07:02:30.172427800Z"
    }
   },
   "id": "e7cc746fc6ad931c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               comment  label\n",
      "0    买书前在网上看完了开放的部分，心里念念不忘，每看到书名都能引起联想，放不下的一本书，一般网络...      1\n",
      "1    penny是骰子，一场麻将下来，人人都有至少抓一把的机会最爱《盛夏光年》@Chloe的呼吸 ...      1\n",
      "2                                 幸福如此简单[爱你][爱你][爱你]\\n      1\n",
      "3    谢谢莫莫 我就怕你戴会小，刚刚好我就放心啦...美美的过年吧...蜜蜜小姐新年快乐思密达.....      1\n",
      "4    回复@MMaiH:这个是花一晚上的钱，住两晚上！[哈哈] 回复@磨西春天酒吧:好的哈[哈哈]...      1\n",
      "..                                                 ...    ...\n",
      "195  我怀疑脑子不正常，说到后面电话都要起来了[抓狂] [抓狂][抓狂] 某人不知道在哪儿看的偏方...      0\n",
      "196                        领导那边比基尼，洛阳现在Moncle羽绒服吖[晕]\\n      0\n",
      "197  突如其来的疼痛！Hold不住了我！[抓狂][泪][生病] 我在:http:俺44-56[晕]...      0\n",
      "198  困死了，，早自习进行中……[泪][泪][泪]诶，为什么大一的孩子那么惨……[左哼哼][右哼哼...      0\n",
      "199                                      又坚强，又心酸 [泪]\\n      0\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data=pd.concat([data_pos[:100],data_neg[:100]], axis=0).reset_index(drop=True)\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:35.025455300Z",
     "start_time": "2023-12-22T07:02:34.931702300Z"
    }
   },
   "id": "a1d1b9b10ceb2225"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X = data.comment.values  # comment\n",
    "y = data.label.values  # label自己给的0 1 2\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:36.898443700Z",
     "start_time": "2023-12-22T07:02:36.843590700Z"
    }
   },
   "id": "9f352673a4e28266"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    # 空列表来储存信息\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # 每个句子循环一次\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  # 预处理语句\n",
    "            add_special_tokens=True,  # 加 [CLS] 和 [SEP]\n",
    "            max_length=MAX_LEN,  # 截断或者填充的最大长度\n",
    "            padding='max_length',  # 填充为最大长度，这里的padding在之间可以直接用pad_to_max但是版本更新之后弃用了，老版本什么都没有，可以尝试用extend方法\n",
    "            return_attention_mask=True  # 返回 attention mask\n",
    "        )\n",
    "\n",
    "        # 把输出加到列表里面\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # 把list转换为tensor\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:39.274093200Z",
     "start_time": "2023-12-22T07:02:39.240183Z"
    }
   },
   "id": "bb3c0b782adfdec8"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:41.036380400Z",
     "start_time": "2023-12-22T07:02:40.900742600Z"
    }
   },
   "id": "f6057f5cfe920c75"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "encoded_comment = [tokenizer.encode(sent, add_special_tokens=True) for sent in data.comment.values]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:43.667344800Z",
     "start_time": "2023-12-22T07:02:43.429979800Z"
    }
   },
   "id": "42ad63bf3f032a3f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "MAX_LEN = max([len(sent) for sent in encoded_comment])\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "test_inputs, test_masks = preprocessing_for_bert(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:47.182945Z",
     "start_time": "2023-12-22T07:02:46.907681Z"
    }
   },
   "id": "338e3b424ed3148f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:48.294971100Z",
     "start_time": "2023-12-22T07:02:48.245105300Z"
    }
   },
   "id": "1cb317c3effec17"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# 给训练集创建 DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# print(train_dataloader)\n",
    "\n",
    "# 给验证集创建 DataLoader\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:50.890032400Z",
     "start_time": "2023-12-22T07:02:50.849142200Z"
    }
   },
   "id": "e3bb2f6de4ce3651"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # 输入维度(hidden size of Bert)默认768，分类器隐藏维度，输出维度(label)\n",
    "        D_in, H, D_out = 768, 100, 2\n",
    "\n",
    "        # 实体化Bert模型\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "        # 实体化一个单层前馈分类器，说白了就是最后要输出的时候搞个全连接层\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),  # 全连接\n",
    "            nn.ReLU(),  # 激活函数\n",
    "            nn.Linear(H, D_out)  # 全连接\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        # 为分类任务提取标记[CLS]的最后隐藏状态，因为要连接传到全连接层去\n",
    "        print(outputs)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        # 全连接，计算，输出label\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        print(logits)\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:36:21.142262100Z",
     "start_time": "2023-12-22T07:36:20.998645300Z"
    }
   },
   "id": "577dee75981afeba"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def initialize_model(epochs=2):\n",
    "    \"\"\"\n",
    "    初始化我们的bert，优化器还有学习率，epochs就是训练次数\n",
    "    \"\"\"\n",
    "    # 初始化我们的Bert分类器\n",
    "    bert_classifier = BertClassifier()\n",
    "    # 用GPU运算\n",
    "    bert_classifier.to(device)\n",
    "    # 创建优化器\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,  # 默认学习率\n",
    "                      eps=1e-8  # 默认精度\n",
    "                      )\n",
    "    # 训练的总步数\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # 学习率预热\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,  # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:54.120394500Z",
     "start_time": "2023-12-22T07:02:54.074517400Z"
    }
   },
   "id": "3cd2808094e71c2e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # 交叉熵\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:55.141665Z",
     "start_time": "2023-12-22T07:02:55.085814Z"
    }
   },
   "id": "bfb39ff52fef593a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader=None, epochs=2, evaluation=False):\n",
    "    # 开始训练循环\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # 表头\n",
    "        print(f\"{'Epoch':^7} | {'每40个Batch':^9} | {'训练集 Loss':^12} | {'测试集 Loss':^10} | {'测试集准确率':^9} | {'时间':^9}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # 测量每个epoch经过的时间\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # 在每个epoch开始时重置跟踪变量\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # 把model放到训练模式\n",
    "        model.train()\n",
    "\n",
    "        # 分batch训练\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            # 把batch加载到GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            #print(b_labels.shape)\n",
    "            # 归零导数\n",
    "            model.zero_grad()\n",
    "            # 真正的训练\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "            #print(logits.shape)\n",
    "            # 计算loss并且累加\n",
    "\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 归一化，防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # 更新参数和学习率\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print每40个batch的loss和time\n",
    "            if (step % 40 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # 计算40个batch的时间\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print训练结果\n",
    "                print(\n",
    "                    f\"{epoch_i + 1:^7} | {step:^10} | {batch_loss / batch_counts:^14.6f} | {'-':^12} | {'-':^13} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # 重置batch参数\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # 计算平均loss 这个是训练集的loss\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation:  # 这个evalution是我们自己给的，用来判断是否需要我们汇总评估\n",
    "            # 每个epoch之后评估一下性能\n",
    "            # 在我们的验证集/测试集上.\n",
    "            test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
    "            # Print 整个训练集的耗时\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "            print(\n",
    "                f\"{epoch_i + 1:^7} | {'-':^10} | {avg_train_loss:^14.6f} | {test_loss:^12.6f} | {test_accuracy:^12.2f}% | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:56.178891Z",
     "start_time": "2023-12-22T07:02:56.129024Z"
    }
   },
   "id": "a5fddd98d073adb"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    在每个epoch后验证集上评估model性能\n",
    "    \"\"\"\n",
    "    # model放入评估模式\n",
    "    model.eval()\n",
    "\n",
    "    # 准确率和误差\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "\n",
    "    # 验证集上的每个batch\n",
    "    for batch in test_dataloader:\n",
    "        # 放到GPU上\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # 计算结果，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)  # 放到model里面去跑，返回验证集的ouput就是一行三列的\n",
    "            # label向量可能性，这个时候还没有归一化所以还不能说是可能性，反正归一化之后最大的就是了\n",
    "\n",
    "        # 计算误差\n",
    "        loss = loss_fn(logits, b_labels.long())\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        # get预测结果，这里就是求每行最大的索引咯，然后用flatten打平成一维\n",
    "        preds = torch.argmax(logits, dim=1).flatten()  # 返回一行中最大值的序号\n",
    "\n",
    "        # 计算准确率，这个就是俩比较，返回相同的个数, .cpu().numpy()就是把tensor从显卡上取出来然后转化为numpy类型的举证好用方法\n",
    "        # 最后mean因为直接bool形了，也就是如果预测和label一样那就返回1，正好是正确的个数，求平均就是准确率了\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "\n",
    "    # 计算整体的平均正确率和loss\n",
    "    val_loss = np.mean(test_loss)\n",
    "    val_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:02:59.270083500Z",
     "start_time": "2023-12-22T07:02:59.249139600Z"
    }
   },
   "id": "195c0b87b98c27e8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\qcru\\IQue\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and testing:\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |     5      |    0.609848    |      -       |       -       |  622.96  \n",
      "--------------------------------------------------------------------------------\n",
      "   1    |     -      |    0.609848    |   0.424712   |    90.00    % |  650.13  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  | 每40个Batch |   训练集 Loss   |  测试集 Loss  |  测试集准确率   |    时间    \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |     5      |    0.288218    |      -       |       -       |  577.71  \n",
      "--------------------------------------------------------------------------------\n",
      "   2    |     -      |    0.288218    |   0.400553   |    80.00    % |  605.35  \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of paramerters in networks is 102344750  \n"
     ]
    }
   ],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "# print(\"Start training and validation:\\n\")\n",
    "print(\"Start training and testing:\\n\")\n",
    "train(bert_classifier, train_dataloader, test_dataloader, epochs=2, evaluation=True)  # 这个是有评估的\n",
    "\n",
    "\n",
    "net = BertClassifier()\n",
    "print(\"Total number of paramerters in networks is {}  \".format(sum(x.numel() for x in net.parameters())))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:24:15.902205500Z",
     "start_time": "2023-12-22T07:03:13.277917Z"
    }
   },
   "id": "cc5a8e8b90ce50e8"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.4093, -0.4306],\n        [ 0.5259, -0.4262],\n        [ 0.1492, -0.2210],\n        [ 0.2906, -0.3334],\n        [ 0.3153, -0.4795],\n        [ 0.3258, -0.2974],\n        [ 0.1630,  0.0341],\n        [ 0.4093, -0.4306],\n        [ 0.3153, -0.4795],\n        [ 0.2906, -0.3334],\n        [ 0.2170, -0.1598],\n        [ 0.2492, -0.2989],\n        [ 0.1096, -0.2520],\n        [ 0.5259, -0.4262],\n        [ 0.3153, -0.4795],\n        [ 0.0818, -0.1715],\n        [ 0.2909, -0.4498],\n        [ 0.3153, -0.4795],\n        [ 0.2359, -0.1729],\n        [ 0.0022, -0.0641],\n        [ 0.1837, -0.1641],\n        [ 0.4093, -0.4306],\n        [ 0.5259, -0.4262],\n        [ 0.4093, -0.4306],\n        [ 0.5259, -0.4262],\n        [ 0.3153, -0.4795],\n        [ 0.0818, -0.1715],\n        [ 0.2909, -0.4498],\n        [ 0.3153, -0.4795],\n        [ 0.2359, -0.1729],\n        [ 0.4051, -0.3189],\n        [ 0.1613, -0.1567],\n        [ 0.1889, -0.3006],\n        [ 0.2185, -0.0952],\n        [ 0.1476, -0.0853],\n        [ 0.3015, -0.2525],\n        [-0.0322, -0.0560],\n        [ 0.2494, -0.1798],\n        [ 0.1072, -0.0282],\n        [ 0.2856, -0.2012],\n        [ 0.3260, -0.2649],\n        [ 0.3663, -0.2977]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids,attention_mask=preprocessing_for_bert(\"Rotary Transformer，简称RoFormer，是我们自研的语言模型之一\")\n",
    "bert_classifier.forward(input_ids,attention_mask)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T07:37:43.329345500Z",
     "start_time": "2023-12-22T07:36:29.072373400Z"
    }
   },
   "id": "56212df1a1d9e6a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T05:48:21.183014900Z",
     "start_time": "2023-12-22T05:48:21.131106200Z"
    }
   },
   "id": "e6d731ea60cb6db9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "11769b4b471a3ac3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
